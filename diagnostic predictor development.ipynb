{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6853541",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that predicts ICD-10 diagnostic label\n",
    "#Representation: Naive Bayes, defined Y=arg max P(Y=yi)DotProd(P(Xi|yi)*P(Xi))\n",
    "#Evaluation: AUC (combination of TPR+FPR). Good for balanced classes.\n",
    "#Predicted label is a ICD-10 insurance code and description (e.g. \"A0100\", \"Cholera due to Vibrio cholerae 01, biovar cholerae\")\n",
    "#Input space is a multinomial bag of n-grams taken from ICD-10 diagnostic descriptions, and weighted by nl(IDF) to favor rare words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b72fa726",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#other functions used in alternative versions of app\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.naive_bayes import BernoulliNB\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from scipy import sparse\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# import swifter\n",
    "# from nltk.metrics.distance import jaccard_distance\n",
    "# from nltk.util import ngrams\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# import psutil\n",
    "# import memory_profiler\n",
    "# import sys\n",
    "# import pandas as pd\n",
    "\n",
    "########CURRENT APP VERSION #########\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92b4473b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1'])\n",
      "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', '_', '__', '___', '_i', '_ii', '_iii', '_i1'])\n"
     ]
    }
   ],
   "source": [
    "print(globals().keys())\n",
    "print(locals().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642b6177",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#multinomial naive bayes model training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tfidf_model \u001b[38;5;241m=\u001b[39m TfidfVectorizer(ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m),lowercase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,analyzer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_wb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m X_train \u001b[38;5;241m=\u001b[39m tfidf_model\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mdata\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#add 1 to every term x document position (prevents 0 probability problem)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m X_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mtoarray()\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#multinomial naive bayes model training\n",
    "tfidf_model = TfidfVectorizer(ngram_range=(2,2),lowercase = True,analyzer='char_wb')\n",
    "X_train = tfidf_model.fit_transform(data['description'])\n",
    "#add 1 to every term x document position (prevents 0 probability problem)\n",
    "X_train = X_train.toarray()+1\n",
    "#transform back to sparse matrix\n",
    "X_train = sparse.csr_matrix(X_train)\n",
    "\n",
    "y_train = data['code']\n",
    "\n",
    "MNB_model = BernoulliNB(fit_prior=False,binarize=.5).fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28a8963a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        code                                        description\n",
      "0       code                                        description\n",
      "1       A000  Cholera due to Vibrio cholerae 01 biovar cholerae\n",
      "2       A009                                Cholera unspecified\n",
      "3      A0100                          Typhoid fever unspecified\n",
      "4      A0101                                 Typhoid meningitis\n",
      "...      ...                                                ...\n",
      "11378  Z9912  Encounter for respirator [ventilator] dependen...\n",
      "11379   Z992                       Dependence on renal dialysis\n",
      "11380   U070                            Vaping-related disorder\n",
      "11381   U071                                           COVID-19\n",
      "11382   U099                Post COVID-19 condition unspecified\n",
      "\n",
      "[11383 rows x 2 columns]\n",
      "{' d': 14, 'de': 312, 'es': 355, 'sc': 677, 'cr': 296, 'ri': 654, 'ip': 450, 'pt': 630, 'ti': 723, 'io': 449, 'on': 596, 'n ': 543, ' c': 13, 'ch': 288, 'ho': 422, 'ol': 594, 'le': 500, 'er': 354, 'ra': 646, 'a ': 210, 'du': 326, 'ue': 744, 'e ': 330, ' t': 30, 'to': 727, 'o ': 576, ' v': 32, 'vi': 768, 'ib': 437, 'br': 263, 'ae': 221, ' 0': 1, '01': 118, '1 ': 119, ' b': 12, 'bi': 256, 'ov': 604, 'va': 766, 'ar': 234, 'r ': 638, ' u': 31, 'un': 751, 'ns': 568, 'sp': 690, 'pe': 618, 'ec': 339, 'ci': 289, 'if': 441, 'fi': 371, 'ie': 440, 'ed': 340, 'd ': 302, 'ty': 735, 'yp': 827, 'ph': 620, 'oi': 591, 'id': 439, ' f': 16, 'fe': 368, 'ev': 358, 've': 767, ' m': 23, 'me': 529, 'en': 350, 'ni': 558, 'in': 448, 'ng': 556, 'gi': 392, 'it': 454, 'is': 453, 's ': 670, ' w': 33, 'wi': 781, 'th': 722, 'h ': 403, ' h': 18, 'he': 414, 'ea': 337, 'rt': 664, 't ': 700, ' i': 19, 'nv': 571, 'vo': 769, 'lv': 515, 'em': 349, 'nt': 569, ' p': 26, 'pn': 625, 'ne': 554, 'eu': 357, 'um': 750, 'mo': 537, 'ia': 436, ' a': 11, 'hr': 425, ' o': 25, 'os': 601, 'st': 694, 'te': 720, 'eo': 351, 'om': 595, 'my': 542, 'ye': 819, 'el': 348, 'li': 504, 'ot': 602, 'co': 293, 'mp': 538, 'pl': 623, 'ic': 438, 'ca': 283, 'at': 236, 'pa': 615, ' s': 29, 'sa': 675, 'al': 228, 'lm': 507, 'll': 506, 'la': 496, ' e': 15, ' l': 22, 'lo': 509, 'oc': 585, 'iz': 458, 'ze': 836, 'nf': 555, 'ct': 298, 'sh': 682, 'hi': 417, 'ig': 442, 'ge': 389, 'si': 683, 'dy': 329, 'ys': 829, 'se': 679, 'ro': 659, 'op': 598, 'og': 589, 'c ': 269, 'i ': 431, 'ox': 606, 'xi': 801, 'as': 235, 'iv': 456, 'oh': 590, 'or': 600, 'rr': 662, 'rh': 653, 'ha': 410, 'ag': 223, 'na': 550, 'l ': 485, 'am': 229, 'py': 634, 'yl': 823, 'ob': 584, 'ba': 250, 'ac': 219, ' y': 35, 'rs': 663, 'cl': 291, 'tr': 729, 'di': 316, 'iu': 455, 'm ': 519, 'ff': 369, 'il': 446, ' r': 28, 're': 650, 'cu': 299, 'ur': 754, 'fo': 375, 'oo': 597, 'od': 586, 'db': 309, 'bo': 261, 'rn': 658, 'ta': 716, 'ap': 232, 'hy': 430, 'cc': 284, 'tu': 732, 'ul': 749, 'sm': 687, 'po': 626, 'so': 689, 'g ': 380, 'rf': 651, 'fr': 376, ' [': 10, '[c': 189, 'we': 779, 'lc': 498, 'ii': 444, 'i]': 435, '] ': 209, 'ut': 756, 'eb': 338, 'ry': 668, 'y ': 810, ' n': 24, 'no': 564, 'nd': 553, 'ma': 526, 'of': 588, 'f ': 363, 'ab': 218, 'bs': 264, 'ce': 286, 'ss': 693, 'lu': 514, 'ai': 225, 'an': 230, 'ou': 603, 'us': 755, 'cy': 300, ' g': 17, 'rd': 649, '[l': 198, 'mb': 527, 'bl': 258, 's]': 674, 'yc': 817, 'pr': 628, 'oz': 608, 'zo': 840, 'oa': 583, 'av': 238, 'ir': 452, 'ga': 387, 'oe': 587, 'rw': 667, 'wa': 776, 'lk': 505, 'k ': 464, 'ad': 220, 'ru': 665, 'ub': 741, 'be': 254, 'rc': 648, 'sy': 698, 'im': 447, 'pi': 621, 'go': 397, 'nc': 552, 'ep': 352, 'rv': 666, ' j': 20, 'jo': 462, 'ts': 730, 'mu': 540, 'sk': 685, 'ke': 472, 'et': 356, 'fl': 372, 'mm': 535, 'rg': 652, 'ly': 517, 'ym': 824, 'ki': 475, 'su': 695, 'bc': 252, 'ey': 361, ' (': 0, '(i': 51, 'nn': 563, 'r)': 641, ') ': 66, '(m': 55, 'mi': 533, 'dd': 311, 'dl': 318, 'e)': 333, 'gl': 394, 'bu': 266, 'gu': 401, 'rm': 657, 'ms': 539, 'pu': 631, 'ax': 240, 'x ': 792, 'ps': 629, 'uc': 742, 'fu': 379, 'do': 321, 't-': 704, '-b': 79, 'ix': 457, '-s': 95, 'tc': 718, 'ex': 360, 'xt': 805, ' z': 36, 'ls': 512, 'ew': 359, 'wh': 780, 'rl': 656, 'm-': 522, '-i': 86, '(d': 46, 'dm': 319, 'c)': 270, 'nu': 570, 'yn': 825, 'ht': 427, 'yo': 826, 'ug': 746, 'gh': 391, 'e-': 334, '-f': 83, 'hs': 426, 'dr': 323, 'tm': 725, 'gr': 398, 'up': 753, 'p ': 609, 'au': 237, '-n': 91, 'eg': 343, '[e': 191, 'e.': 335, '. ': 100, 'bd': 253, 'fa': 367, \"s'\": 671, \"' \": 39, 'np': 565, '[p': 202, 'r]': 645, 'ck': 290, 'az': 242, 'zi': 839, 'rp': 660, 'wo': 785, 'nz': 575, 'za': 835, 'ju': 463, 'uv': 758, 'tt': 731, \"n'\": 544, \"'s\": 41, 'hu': 428, 'ee': 341, 'da': 308, 'ao': 231, \"t'\": 701, '(t': 61, 'ow': 605, 'hl': 419, 'yd': 818, '(v': 63, 'm)': 521, 'xu': 806, 'ua': 740, 'ui': 747, 'vu': 770, 'sv': 696, 'tl': 724, 'ya': 815, 'aw': 239, 'ws': 788, 'lt': 513, 'b ': 243, 'rk': 655, 'xe': 798, 'k-': 466, 'nj': 559, 'ek': 347, 'ud': 743, '[b': 188, \"l'\": 486, 'e]': 336, 'eh': 344, ' q': 27, 'q ': 635, 'lp': 510, 'ak': 227, 'ka': 470, 'u ': 737, '[a': 187, 'a.': 214, 'yt': 830, 'm]': 525, '-a': 78, 'tz': 736, 'zf': 837, 'ld': 499, 'dt': 325, '-j': 87, 'ja': 459, 'ko': 479, 'uk': 748, ' k': 21, 'ku': 482, 'n-': 547, 'sl': 686, 'r-': 643, 'ei': 345, 'nk': 560, 'mn': 536, 'rb': 647, 'eq': 353, 'qu': 637, 'sq': 691, 'o-': 580, '[r': 203, 'g-': 383, 'd-': 305, 'xa': 796, 'ik': 445, 'ny': 574, \"o'\": 577, \"'n\": 40, 'ez': 362, 'zu': 841, 'ft': 378, 'df': 313, 'w ': 772, '-c': 80, 'ky': 484, 'cz': 301, 'by': 268, 'wp': 786, '[m': 199, \"r'\": 639, 'xv': 807, '[s': 204, '[f': 192, ' 6': 7, '6 ': 175, 'a-': 213, '-(': 74, '(s': 60, 'ef': 342, '[h': 194, 'v]': 765, 'ah': 224, '(e': 47, 'l)': 488, 'lg': 502, '(c': 45, 'o)': 579, ')-': 68, '-p': 93, 'hp': 423, 'hc': 412, 'cp': 294, 'uu': 757, 'yr': 828, 'zy': 842, 'yg': 820, 'hm': 420, 'af': 222, 'xo': 802, 'lb': 497, 'i)': 433, '[u': 206, 'wu': 790, 'ok': 593, 'kw': 483, 'gy': 402, 'gn': 396, 'yi': 822, 'n]': 549, 'm.': 523, 'kl': 476, '[k': 197, 'k.': 467, 'c]': 282, 'o1': 581, '15': 128, '57': 174, '7 ': 179, 'h.': 407, 's)': 672, '(a': 43, 'a)': 212, '(p': 58, 'b.': 247, 'c.': 272, 'ln': 508, 'xs': 804, 's-': 673, 'v-': 764, ' 2': 3, '2 ': 134, 'v ': 762, '2]': 148, 'pp': 627, \"i'\": 432, 'y)': 812, 'dg': 314, 'gk': 393, 'b-': 246, 't/': 705, '/n': 109, '-h': 85, 'k/': 468, '/t': 111, 'y-': 813, '-t': 96, '-l': 89, 'cd': 285, 'd3': 306, '30': 152, '0-': 115, 'vy': 771, 'a]': 216, 'a/': 215, '/l': 107, '(h': 50, '-1': 75, '1-': 121, 'd)': 304, 'r/': 644, '/a': 104, 'l-': 489, 'c/': 273, '/m': 108, 'nm': 562, 'gg': 390, 'bm': 259, 'b1': 248, '12': 125, '-6': 77, '6-': 176, '[g': 193, 'g6': 385, '6p': 178, 'pd': 617, 'd]': 307, 'pf': 619, 'fh': 370, 'h]': 409, 'hb': 411, 'c-': 271, '-u': 97, 'xy': 809, '-m': 90, 'cq': 295, '[d': 190, \"d'\": 303, ' x': 34, 'nh': 557, 'sf': 680, '[i': 195, 'ih': 443, 'lh': 503, 't)': 703, 'tp': 728, 'gm': 395, 'sg': 681, \"f'\": 364, 'p]': 614, 'aj': 226, '-d': 81, \"e'\": 331, 'bn': 260, 'lf': 501, '1]': 133, '-v': 98, ' 1': 2, 'yx': 832, '-e': 82, '(n': 56, 'kh': 474, 'hh': 416, \"g'\": 381, '-r': 94, 'uf': 745, 'xc': 797, 'ay': 241, 'ds': 324, 'pm': 624, 'yb': 816, 'b6': 249, 'x-': 793, 'p-': 612, ' 3': 4, '3-': 150, 'n/': 548, '/v': 112, 'oy': 607, 'nb': 551, 'yh': 821, 'lw': 516, 'dh': 315, '3 ': 149, 'tn': 726, \"p'\": 610, '-k': 88, 'mc': 528, 'm2': 524, '(-': 42, 'n)': 546, 'kr': 480, 'bb': 251, 'rq': 661, 'n(': 545, 'h-': 406, 'ml': 534, 'i-': 434, '-o': 92, 'z ': 834, 'jj': 461, '1)': 120, '2)': 135, 'kn': 478, 'wn': 784, 'nx': 573, 'sr': 692, 'hn': 421, 'je': 460, 'y]': 814, 'g)': 382, 'dj': 317, '/r': 110, 'pw': 633, '[n': 200, 'ej': 346, 'xp': 803, 'xh': 800, 'p1': 613, 'cs': 297, 'wr': 787, 't]': 715, 'gt': 400, 'dn': 320, '[w': 208, 'fm': 373, 'sz': 699, 'gs': 399, 'f-': 366, 'ks': 481, 'lz': 518, 'zh': 838, \"k'\": 465, \"h'\": 404, 'wy': 791, 'o]': 582, '(f': 48, ' 5': 6, '5 ': 171, '-g': 84, 'dp': 322, 'h)': 405, 'fs': 377, \"m'\": 520, 'tv': 733, 'nl': 561, '(l': 54, '(o': 57, 'sj': 684, 'wm': 783, '(j': 52, 'ux': 759, \"a'\": 211, '(g': 49, '(b': 44, 'w-': 775, 'xf': 799, 'aq': 233, 'sd': 678, 'hw': 429, '[o': 201, '[t': 205, 'sb': 676, 'bj': 257, 'mf': 530, 'nq': 566, 'sw': 697, 'bp': 262, 'fn': 374, 't(': 702, 'nr': 567, 'qt': 636, 'uo': 752, '(u': 62, 'tw': 734, '-w': 99, 'bg': 255, '[v': 207, 'oj': 592, '(r': 59, 'f)': 365, 'tg': 721, 'wb': 777, 'sn': 688, 'x]': 795, '10': 123, '0 ': 113, 'l]': 495, 'oq': 599, 'p)': 611, \"u'\": 738, 'cn': 292, 'mh': 532, 'uz': 761, 'zz': 843, 'kd': 471, 'td': 719, \"y'\": 811, '[j': 196, ')a': 69, '(w': 64, 'r(': 640, 'e(': 332, 'wd': 778, 'u]': 739, ')s': 71, 'o(': 578, ')m': 70, 'tb': 717, 'c4': 278, '4-': 163, 'c5': 279, 'uy': 760, 'wt': 789, 'k]': 469, 'rz': 669, 'cg': 287, '/d': 106, '24': 142, '4 ': 162, 'mv': 541, '37': 159, '39': 161, '9 ': 184, \"b'\": 244, 'pk': 622, \"w'\": 773, 'hd': 413, 'hk': 418, 'aa': 217, 'b(': 245, '21': 139, '18': 131, '8 ': 182, '13': 126, ' 4': 5, 'x/': 794, '45': 168, '/4': 103, '46': 169, 'xx': 808, '47': 170, 'yy': 833, 'yu': 831, 'hg': 415, 'pv': 632, 'v)': 763, 'g]': 386, 'mg': 531, 'kg': 473, 'c0': 274, '0/': 116, '/c': 105, 'c1': 275, '1/': 122, 'c2': 276, '2/': 137, 'c3': 277, '3/': 151, '4/': 164, '5/': 173, 'c6': 280, '6/': 177, 'c7': 281, '7/': 181, 't1': 706, 't5': 710, '5-': 172, 't6': 711, 't7': 712, '7-': 180, 't8': 713, 't9': 714, '9-': 185, '11': 124, 't2': 707, 't3': 708, 't4': 709, '8/': 183, 'l1': 490, 'l2': 491, 'l3': 492, 'l4': 493, 'l5': 494, 'l(': 487, '2-': 136, 'iq': 451, '00': 117, '0%': 114, '%-': 38, '-2': 76, '20': 138, '% ': 37, 'dc': 310, 'bt': 265, 'dv': 327, 'h2': 408, '- ': 73, ')(': 67, 'km': 477, 'lr': 511, '(k': 53, 'hq': 424, 'w)': 774, 'wl': 782, 'pb': 616, 'g/': 384, '/1': 102, 'dw': 328, 'bw': 267, '(z': 65, 'gb': 388, ' 8': 8, 'r+': 642, '+]': 72, '19': 132, ' 9': 9, '14': 127, '16': 129, '17': 130, '22': 140, '23': 141, '25': 143, '26': 144, '27': 145, '28': 146, '29': 147, '31': 153, '32': 154, '33': 155, '34': 156, '35': 157, '36': 158, '38': 160, '40': 165, '41': 166, '42': 167, 'nw': 572, '9.': 186, '.9': 101}\n",
      "[[1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.27880858 1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " ...\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]\n",
      " [1.         1.         1.         ... 1.         1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(data)\n",
    "print(tfidf_model.vocabulary_)\n",
    "print(X_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43f3cb52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNB_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNB_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[0;32m----> 4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mMNB_model\u001b[49m, outp, pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[1;32m      7\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(tfidf_model, outp, pickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MNB_model' is not defined"
     ]
    }
   ],
   "source": [
    "#Model saving\n",
    "with open('MNB_model', 'wb') as outp:\n",
    "    pickle.dump(MNB_model, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('tfidf_model', 'wb') as outp:\n",
    "    pickle.dump(tfidf_model, outp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc89a9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\n\u001b[1;32m      4\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/seanmiller/Downloads/Untitled spreadsheet - icd10cm_codes_2022 (1).csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     ,sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m     ,engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m     ,names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNB_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[0;32m---> 10\u001b[0m     MNB_model \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m outp:\n\u001b[1;32m     13\u001b[0m     tfidf_model \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(outp)\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "#model & data loading\n",
    "\n",
    "data = pd.read_csv(\n",
    "    filepath_or_buffer = '/Users/seanmiller/Downloads/Untitled spreadsheet - icd10cm_codes_2022 (1).csv'\n",
    "    ,sep=','\n",
    "    ,engine='python'\n",
    "    ,names=['code','description'])\n",
    "\n",
    "with open('MNB_model', 'rb') as outp:\n",
    "    MNB_model = pickle.load(outp)\n",
    "    \n",
    "with open('tfidf_model', 'rb') as outp:\n",
    "    tfidf_model = pickle.load(outp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a5944de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "# sample_data = np.array(['Central Euopean tick-rne encephalitis', #A841\n",
    "#                         'Cntral Euopean tick-rne encephalitis',\n",
    "#                         'Central tick-rne encephalitis',\n",
    "#                         'Central encephalitis',\n",
    "#                         'Cenoral encaphalitis',\n",
    "#                        'Shigellosis due to Shagella dysenteriae', #A030\n",
    "#                        'Shigellosis due to dysenteriae',\n",
    "#                        'Shigellosis due to Shagella dasenteriae',\n",
    "#                        'Shigellosis dysenteriae',\n",
    "#                        'Shigellosis dasenteriae',])\n",
    "\n",
    "def diag_predict(text):\n",
    "\ttry:\n",
    "\t\ttext = np.array([text])\n",
    "\t\tX_test = tfidf_model.transform(text)\n",
    "\t\t#add 1 to every term x document position (prevents 0 probability problem)\n",
    "\t\tX_test = X_test.toarray()+1\n",
    "\t\t#transform back to sparse matrix\n",
    "\t\tX_test = sparse.csr_matrix(X_test)\n",
    "\t\ty_test_predicted = MNB_model.predict(X_test)[0]\n",
    "\t\treturn y_test_predicted, data[data['code']==y_test_predicted]['description'].iloc[0]\n",
    "\texcept Exception as err:\n",
    "\t\treturn str(err)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ec503fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model testing\n",
    "text='Shagella Shigellosis dasenteriae treatment in the right to own a pillow for sure'\n",
    "text='Sepsis due to unspecified staphylococcus'\n",
    "#text='Dislocated jaw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71d0b8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diag_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m icd10_code, icd10_description \u001b[38;5;241m=\u001b[39m \u001b[43mdiag_predict\u001b[49m(text)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(icd10_code)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(icd10_description)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'diag_predict' is not defined"
     ]
    }
   ],
   "source": [
    "icd10_code, icd10_description = diag_predict(text)\n",
    "print(icd10_code)\n",
    "print(icd10_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9a35ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternative: jaccard distance on documents\n",
    "\n",
    "def diag_predict_jacc(text):\n",
    "    jacc = []\n",
    "    gram_num = 2\n",
    "    rows,columns = data.shape\n",
    "    for i in range(0,rows):\n",
    "        jacc.append(jaccard_distance(set(ngrams(data['description'].iloc[i],gram_num)),\n",
    "                                     set(ngrams(text,gram_num))))\n",
    "    data['jacc'] = jacc\n",
    "    data_sorted = data.sort_values(by='jacc',ascending=True).iloc[0]\n",
    "    print(data_sorted)\n",
    "    return data_sorted['code'],data_sorted['description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "87f65066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dislocated jaw\n",
      "code                           I401\n",
      "description    Isolated myocarditis\n",
      "jacc                           0.72\n",
      "Name: 3202, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('I401', 'Isolated myocarditis')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(text)\n",
    "diag_predict_jacc(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "63e69c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#alternative: jaccard distance on words prediction\n",
    "#[CONCLUSION]: Works for short input strings. For long ones, becomes much less accurate and computationally infeasible.\n",
    "\n",
    "#making more efficient:\n",
    "# - skip label if first ~2 train tokens produce >= x distance \n",
    "# - skip label if longest word has >= x distance to any test word\n",
    "# - remove meaningless words and symbols from both train and test set\n",
    "\n",
    "#tokenize the train words just once on page load\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokenizer.tokenize('Eighty-seven miles to go, yet.  Onward!')\n",
    "\n",
    "# A function that returns the length of the value:\n",
    "def set_count(s):\n",
    "    return len(s)\n",
    "\n",
    "def list_of_tokens_as_sets(text):\n",
    "    set_list = [set(w) for w in tokenizer.tokenize(text) if not w.lower() in stop_words]\n",
    "    set_list.sort(key=set_count,reverse=True)\n",
    "    return set_list\n",
    "    \n",
    "data['tokens'] = data.apply(lambda row: list_of_tokens_as_sets(row['description']), axis=1)\n",
    "#function for calculating avg of best matches of train against test, given a single training observation\n",
    "def avg_min_dist(tokens_train,tokens_test):\n",
    "    tokens_train_min_distance = []\n",
    "    for i, single_train in enumerate(tokens_train):\n",
    "        jacc = []\n",
    "        #and for each token in the test data...\n",
    "        for single_test in tokens_test:\n",
    "            #check distance (note train is already a set)\n",
    "            jacc.append(jaccard_distance(single_test,single_train))\n",
    "        #find best match\n",
    "        min_match = np.nanmin(jacc)\n",
    "        #if it's the longest word in training data\n",
    "        if i ==1:\n",
    "            #append the distance if it's a good match\n",
    "            if min_match <= .2:\n",
    "                tokens_train_min_distance.append(np.nanmin(jacc))\n",
    "                continue\n",
    "            #otherwise, break loop and don't consider as a potential winner if not good\n",
    "            else:\n",
    "                tokens_train_min_distance.append(1)\n",
    "                break\n",
    "        #if not longest word in training data, continue calculating as normal\n",
    "        else:\n",
    "            tokens_train_min_distance.append(np.nanmin(jacc))\n",
    "    return np.nanmean(tokens_train_min_distance)\n",
    "\n",
    "\n",
    "def diag_predict_jacc_w(text):\n",
    "    tokens_test = list_of_tokens_as_sets(text)\n",
    "    jacc = []\n",
    "    rows, columns = data.shape\n",
    "    data['avg_min_jacc'] = data.swifter.allow_dask_on_strings(enable=True).apply(lambda row: avg_min_dist(row['tokens'],tokens_test), axis=1)\n",
    "    #data['avg_min_jacc'] = data['tokens'].swifter.apply(lambda tokens_train: avg_min_dist(tokens_train,tokens_test))\n",
    "    #data['avg_min_jacc'] = data.apply(lambda row: avg_min_dist(row['tokens'],tokens_test), axis=1)\n",
    "    data_sorted = data.sort_values(by='avg_min_jacc',ascending=True).iloc[0]\n",
    "    #print(data.sort_values(by='avg_min_jacc',ascending=True).head())\n",
    "    return data_sorted['code'],data_sorted['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "68de8a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83ef677ef3d4665bd199153a206a80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dask Apply:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.01 s, sys: 263 ms, total: 1.27 s\n",
      "Wall time: 8.29 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Z9079', 'Acquired absence of other genital organ(s)')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#text='Shagella Shigellosis dasenteriae treatment in the right to own a pillow for sure'\n",
    "text='Acquired absence of ovaries bilateral'\n",
    "text='Acquired absence of other genital organ(s)'\n",
    "diag_predict_jacc_w(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "804717ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative: One sided jaccard - existence of train words in test; csm_matrix for sparse data to allow upload to gcloud\n",
    "#Conclusion: Numba paralellization needs to be turned off due to lack of compatability with csm_matrix package; this makes\n",
    "#the algorithm much less efficient\n",
    "\n",
    "tfidf_model = TfidfVectorizer(ngram_range=(3,3),lowercase = True,analyzer='char_wb')\n",
    "X_train = tfidf_model.fit_transform(data['description'])\n",
    "\n",
    "#save bag of words model and training data\n",
    "\n",
    "with open('tfidf_model', 'wb') as out1, open('X_train', 'wb') as out2:\n",
    "    pickle.dump(tfidf_model, out1, pickle.HIGHEST_PROTOCOL)\n",
    "    pickle.dump(X_train, out2, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    \n",
    "with open('tfidf_model', 'rb') as out1, open('X_train', 'rb') as out2:\n",
    "    tfidf_model = pickle.load(out1) \n",
    "    X_train = pickle.load(out2)\n",
    "\n",
    "#utility: similarity measurement; paralellized\n",
    "#@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def similarity_func(X_train,X_test): # Function is compiled to machine code when called the first time\n",
    "    pct_train_shared = []\n",
    "    rows, columns = X_train.shape\n",
    "    for i in range(0,rows):   # Numba likes loops\n",
    "        #transform each row of X_train to dense matrix to allow numpy pairwise conditional logic\n",
    "        X_train_row_dense = X_train.getrow(i).toarray()\n",
    "        #append similarity measure: Both > 0 / Train(i) > 0\n",
    "        pct_train_shared.append((np.sum((X_train_row_dense > 0) & (X_test > 0)) / np.sum(X_train_row_dense > 0)))  # Numba likes NumPy functions\n",
    "    return pct_train_shared\n",
    "\n",
    "#each time space is pressed\n",
    "def diag_predict_y_in_x(text):\n",
    "    #transform test data into bag of words (as csm)\n",
    "    text = np.array([text])\n",
    "    X_test = tfidf_model.transform(text).toarray()\n",
    "    #run similarity func\n",
    "    data['similarity'] = similarity_func(X_train,X_test)\n",
    "    data_sorted = data.sort_values(by='similarity',ascending=False)\n",
    "    return data_sorted['code'].iloc[0],data_sorted['description'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2945a57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('B20', 'Human immunodeficiency virus [HIV] disease')\n",
      "('B250', 'Cytomegaloviral pneumonitis')\n",
      "('B251', 'Cytomegaloviral hepatitis')\n",
      "('B252', 'Cytomegaloviral pancreatitis')\n",
      "('B258', 'Other cytomegaloviral diseases')\n",
      "('B259', 'Cytomegaloviral disease unspecified')\n",
      "('B260', 'Mumps orchitis')\n",
      "('B261', 'Mumps meningitis')\n",
      "CPU times: user 32.6 s, sys: 108 ms, total: 32.7 s\n",
      "Wall time: 32.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = ['Huan immunoficiency virus [HIV] disease and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Cytomgaloviral pneumnitis and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Cytomegaoviral hepaitis and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Cytomgaloviral pacreatitis and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Other ctomegaloviral diseases and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Cytomegloviral isease unpecified and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Mumps orhitis and patient was earnest in telling me that their grandmother was sick before she got there',\n",
    "'Mups menngitis and patient was earnest in telling me that their grandmother was sick before she got there']\n",
    "\n",
    "for j in texts:\n",
    "    print(diag_predict_y_in_x(j))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec04f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PART 1: LOADING / SAVING\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from numba import jit\n",
    "import pickle as pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#generate initial data and bag of words model\n",
    "data = read_csv(\n",
    "    filepath_or_buffer = './static/icd10_all_codes.csv'\n",
    "    ,sep=','\n",
    "    ,engine='python')\n",
    "\n",
    "tfidf_model = TfidfVectorizer(ngram_range=(4,4),lowercase = True,analyzer='char_wb')\n",
    "X_train = tfidf_model.fit_transform(data['description'])\n",
    "\n",
    "#save bag of words model and training data\n",
    "\n",
    "with open('tfidf_model', 'wb') as out:\n",
    "    pickle.dump(tfidf_model, out, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('X_train', 'wb') as out:\n",
    "    pickle.dump(X_train, out, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('data', 'wb') as out:\n",
    "    pickle.dump(data, out, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "#load data\n",
    "\n",
    "def load_data():\n",
    "    if ('X_train_csm_column' not in globals().keys()) & ('X_train_index_pointer' not in globals().keys()):\n",
    "        with open('X_train', 'rb') as out2:\n",
    "            X_train  = pickle.load(out2)\n",
    "            globals()['X_train_csm_column'] = np.array(X_train.indices)\n",
    "            globals()['X_train_index_pointer'] = np.array(X_train.indptr)\n",
    "            del X_train\n",
    "    if 'tfidf_model' not in globals().keys():\n",
    "        with open('tfidf_model', 'rb') as out1:\n",
    "            globals()['tfidf_model'] = pickle.load(out1) \n",
    "        #transform test data into bag of words (as csm)\n",
    "    if 'data' not in globals().keys():\n",
    "        with open('data', 'rb') as out2:\n",
    "            globals()['data']  = pickle.load(out2)\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "            \n",
    "load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fae66f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FINAL PART 2: USING SPARSE + PARALELLIZATION by implementing a custom numpy based sparse matrix\n",
    "#THIS CELL IS FUNCTIONS THAT RUN JUST IN TIME IN WEB APP\n",
    "@jit(nopython=True) # Set \"nopython\" mode for best performance, equivalent to @njit\n",
    "def similarity_func(X_train_csm_column\n",
    "                    ,X_train_index_pointer\n",
    "                    ,X_train_sparse_row_num\n",
    "                    ,X_test): # Function is compiled to machine code when called the first time\n",
    "    \n",
    "    #initialize storage\n",
    "    X_train_shared_tokens_compressed = []\n",
    "    X_train_shared_tokens = []\n",
    "    X_train_total_tokens = []\n",
    "    \n",
    "    #create 1d array of length equal to compressed matrix rows containing 1 if that row (> 0 x_train token) has a match in test\n",
    "    for X_train_col in X_train_csm_column:   # Numba likes loops\n",
    "        X_train_shared_tokens_compressed.append(X_test[X_train_col] > 0)\n",
    "        \n",
    "    #create 1d array of length equal to original matrix rows containing sum of total and intersecting tokens\n",
    "    for row_sparse in range(X_train_sparse_row_num):\n",
    "        total = 0\n",
    "        intersections = 0\n",
    "        #for each pointer, iterate through the compressed matrix to add up total & total intersections per row\n",
    "        for row_compressed in range(X_train_index_pointer[row_sparse],X_train_index_pointer[row_sparse+1]):\n",
    "            intersections += X_train_shared_tokens_compressed[row_compressed]\n",
    "            total += 1\n",
    "        #then insert within arrays that have length of sparse matrix rows\n",
    "        X_train_total_tokens.append(total)\n",
    "        X_train_shared_tokens.append(intersections)\n",
    "        #then return similarity\n",
    "    return (np.array(X_train_shared_tokens) / np.array(X_train_total_tokens), np.array(X_train_total_tokens))\n",
    "\n",
    "#each time space is pressed\n",
    "def diag_predict_y_in_x(text):\n",
    "    #load global variables as needed\n",
    "    #load_data()\n",
    "    X_test = tfidf_model.transform(np.array([text])).toarray()\n",
    "    tr,tc = X_test.shape\n",
    "    X_test = X_test.reshape(tc,)\n",
    "    #create a numpy friendly 3 array compressed sparse matrix\n",
    "    X_train_sparse_row_num = len(data['code']) #index pointer has length 1 longer than rows in sparse matrix\n",
    "    #load data and run similarity\n",
    "    data['similarity'], data['total_tokens'] = similarity_func(X_train_csm_column\n",
    "                                                               ,X_train_index_pointer\n",
    "                                                               ,X_train_sparse_row_num\n",
    "                                                               ,X_test) \n",
    "    data['similarity_adjusted'] = data['similarity']*(np.log(data['total_tokens']+10) / np.log(2))\n",
    "    #sort data twice in output (prioritizing memory over processing)\n",
    "    return data.sort_values(by='similarity_adjusted',ascending=False)['code'].iloc[0],data.sort_values(by='similarity_adjusted',ascending=False)['description'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e258728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('S0302XS', 'Dislocation of jaw left side sequela')\n",
      "('S0302XS', 'Dislocation of jaw left side sequela')\n",
      "('B360', 'Pityriasis versicolor')\n",
      "('B360', 'Pityriasis versicolor')\n",
      "('B0802', 'Orf virus disease')\n",
      "('B977', 'Papillomavirus as the cause of diseases classified elsewhere')\n",
      "('C269', 'Malignant neoplasm of ill-defined sites within the digestive system')\n",
      "('C269', 'Malignant neoplasm of ill-defined sites within the digestive system')\n",
      "('M62838', 'Other muscle spasm')\n",
      "('S66393A', 'Other injury of extensor muscle fascia and tendon of left middle finger at wrist and hand level initial encounter')\n",
      "('Z9852', 'Vasectomy status')\n",
      "('Z9852', 'Vasectomy status')\n",
      "('A009', 'Cholera unspecified')\n",
      "('A009', 'Cholera unspecified')\n",
      "('T86832', 'Bone graft infection')\n",
      "CPU times: user 1.11 s, sys: 23.6 ms, total: 1.13 s\n",
      "Wall time: 1.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "texts = ['Patient in car accident with dislocation at left side of jaw...'\n",
    "        ,'Dislocation of jaw ' #should be Dislocation of jaw unspecified side initial encounter\n",
    "        ,'Pityriasis versicolor'\n",
    "        ,'Patient has rash on chest suggesting Pityriasis versicolor'\n",
    "        ,'Papillomavirus causing disease...' #should be Pityriasis versicolor'\n",
    "        ,'Papillomavirus as the cause of diseases classified elsewhere' #should be Pityriasis versicolor\n",
    "        ,'Patient has a Malignant neoplasm within digestive system...'\n",
    "        ,'Malignant neoplasm of ill-defined sites within the digestive system'\n",
    "        ,'Other injury of extensor muscle fascia'\n",
    "        ,'Other injury of extensor muscle fascia and tendon of left middle finger at wrist and hand level initial encounter'\n",
    "        ,'Vasectomy status'\n",
    "        ,'The patient has a positive Vasectomy status'\n",
    "        ,'Cholera unspecified'\n",
    "        ,'Saw patient with Cholera unspecified '\n",
    "        ,'Gonnocal infection ' #should be\n",
    "        ] \n",
    "\n",
    "for j in texts:\n",
    "    print(diag_predict_y_in_x(j))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89d499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5244c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
